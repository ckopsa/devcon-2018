#+TITLE: Voice Synthesis with TensorFlow
#+DATE: <2018-09-17 Mon>
#+AUTHOR: Colton Kopsa
#+EMAIL: coljamkop@gmail.com

#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:t
#+OPTIONS: reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:800
#+OPTIONS: toc:1
#+REVEAL_ROOT: file:///home/colton/dev/reveal.js/
#+REVEAL_THEME: moon
#+REVEAL_HLEVEL: 1
#+REVEAL_PLUGINS: (markdown notes)

* About Me
  Colton Kopsa
  
  Clearwater Analytics
  
  BYU-I
  #+BEGIN_NOTES
  Software Developer

  Novice in Deep Learning/Machine Learning

  Spent a good amount of time learning about this subject
  
  I may not be able to answer all questions

  Feel free to correct me
  #+END_NOTES

* Voice Synthesis
  - Concatenation Based Synthesis
  - Statistical Parametric Synthesis
  - Deep Learning Based Synthesis
** Concatenation Based Synthesis 
   #+ATTR_REVEAL: :frag (roll-in)
   - Speech -> unique sounds
   - Phrases -> unique symbols
   - Words -> unique symbols -> unique sounds
   #+BEGIN_NOTES
   - This has been drastically simplified.
   - Dataset built from audio-sentence(phrase) pairs
   - Unique sounds (phonemes, diphones, etc) that are a part of our speech
   - Unique symbols are typically accompanied by metadata:
     - Preceding and following phonemes
     - Position of segment in syllable
     - Position of syllable in word & phrase
     - Position of word in phrase
     - Stress/accent/length features of current/preceding/following syllables
     - Distance from stressed/accented syllable
     - POS of current/preceding/following word
     - Length of current/preceding/following phrase
     - End tone of phrase
     - Length of utterance measured in syllables/words/phrases

   The list of unique sounds are stitched together
   #+END_NOTES

** Concatenation Based Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="concat-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Statistical Parametric Synthesis
   #+ATTR_REVEAL: :frag (roll-in)
   - Uses Hidden Markov Models
   - Each generates different parts of the final waveform
   #+BEGIN_NOTES
   Similar to Concatenation
   Database is replaced with a trained model
   #+END_NOTES
    
** Statistical Parametric Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="sp-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Deep Learning Based Synthesis
   #+ATTR_REVEAL: :frag (roll-in)
   - Machine Learning Magic

** Deep Learning Based Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="deep-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Questions?
* What is Deep Learning?
  Machine Learning -> Neural Networks -> Deep Learning
** Machine Learning (Supervised Learning)
   - Classical Programming vs Machine Learning
     - Classical Programming: Data + Rules => Answers
     - Machine Learning: Data + Answers => Rules
** Neural Networks
   #+REVEAL_HTML: <iframe width="1200" height="600" src="https://www.youtube.com/embed/rEDzUT3ymw4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
** How does it learn?
   1. Data -> NN -> Prediction
   2. Loss = Prediction - Truth
   3. Optimizer uses loss to nudge weights in the right direction.
   4. Accuracy improves.
** Deep Learning
   Neural Network with many hidden layers
** ImageNet
   #+ATTR_REVEAL: :frag (roll-in)
     - 1.4 million images
     - 1000 categories
     - 2011 - Classical Approach - 74.3%
     - 2012 - Deep Learning - 83.6%
     #+BEGIN_NOTES
     - ImageNet was a competition that popularized deep learning
     - AlexNet blew the previous competition out of the water
     - Deep Learning has ruled the competition since
     #+END_NOTES

** ImageNet Results
   [[file:ImageNet%20Results.png]] 
   #+BEGIN_NOTES
   - 2010 and 2011 used shallow methods
   - 2012 is when AlexNet won with 8 layers
   - 2013 was similar to AlexNet but with improved training
   - 2014 moves to 22 layers with a jump of 5%
   - 2015 jumps 3% with 152 layers
   #+END_NOTES
   
** Why is deep learning better?
   #+ATTR_REVEAL: :frag (roll-in)
   - More layers = More capability to memorize
   - No Feature Engineering
   - Train as one model
   #+BEGIN_NOTES
   - As trainable weight increase it's ability to memorize the data it's
     training on improves. The more layers the better.
   - Feature engineering is used with other machine learning methods
     - A models ability to infer is greatly influenced by the data it trains on
     - Including irrelevant data can cause the models ability to infer to
       degrade
     - The person training the model has to deliberately choose which features
       to include/discard
     - Neural networks learn which features to stress/ignore while it adjusts
       its weights
   #+END_NOTES

** Tying it Back To Voice Synthesis
   - What we have:
     - Text (Data)
     - Audio (Answers)
   - What we want:
     - A way to convert text to audio (Rules)
   #+BEGIN_NOTES
   Other voice synthesis processes usually accompany the text data with
   metadata, this model will operate without metadata.
   #+END_NOTES
** Questions?
* Tacotron - Deep Learning for Voice Synthesis
  [[file:tacotron-architecture.png]] 
** Inputs
   Unique Id Representation of Characters

   'a' -> 1, 'b' -> 2
** Outputs:
   Log-Mel Spectrograms
   
   Linear Spectrograms

   https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png

** Character Embeddings
   #+ATTR_REVEAL: :frag (roll-in)
   - Gives spatial significance to the characters
   - Map lower dimensional data to a higher dimension
   #+REVEAL: split
   #+REVEAL_HTML: <img width="750" height="750" src="https://www.tensorflow.org/images/tsne.png">
   #+BEGIN_NOTES
   - System and computer are close
   - Data and information are close
   - English and French are close
   #+END_NOTES
   #+REVEAL: split
   #+BEGIN_SRC python
     embedding = keras.layers.Embedding(input_dim=vocab_inp_size,
                                        output_dim=256,
                                        input_length=max_length_inp)
   #+END_SRC
   #+BEGIN_NOTES
   vocab inp size - the total number of unique characters in our dataset
   max length inp - the max number of characters in a sentence from our dataset
   #+END_NOTES
** "Pre-net"
   "Helps convergence and improves generalization."
   #+BEGIN_SRC python
     class EncoderPrenet(keras.Model):
       def __init__(self):
         super(EncoderPrenet, self).__init__()
         self.dense_1 = keras.layers.Dense(256, activation=tf.nn.relu)
         self.dense_2 = keras.layers.Dense(128, activation=tf.nn.relu)
         self.dropout = tf.keras.layers.Dropout(0.5)

       def call(self, x):
         x = self.dense_1(x)
         x = self.dropout(x)
         x = self.dense_2(x)
         x = self.dropout(x)
         return x
   #+END_SRC
   #+BEGIN_NOTES
   Dropout randomly drop data given a dropout rate. In this case %50. This can
   help generalize because it learns to work without information.
   
   I think that the convergence benefits come with the non-Linear (ReLU) to help
   normalize the data in the model.
   #+END_NOTES
** CBHG
   Convolutional Banks, Highway Network, Bidirectional GRU
   "Powerful module for extracting representations from sequences"
   [[file:cbhg.png]]
   #+BEGIN_NOTES
   Lots of new stuff, stay with me.
   #+END_NOTES
*** Convolutional Neural Networks - CNN
    #+ATTR_REVEAL: :frag (roll-in)
    - Trains on smaller parts
    - Applies filters
    - Typically Paired with a pooling layer
    #+BEGIN_NOTES
    - Dense Networks struggle to focus on details
    - Conv Nets are a way to focus on details under different lights
    - It focuses on details by stepping through the data frame-by-frame
    - It gets different lights by applying different filters on the data
    #+END_NOTES
      
*** Convolutional Neural Networks - CNN (Example)
    https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif
    #+BEGIN_NOTES
    The box traversing the image is stepping through the data frame-by-frame
    The images that it generated on the left is the filtered data
    #+END_NOTES
*** Convolutional Neural Networks - CNN (Code)
    #+BEGIN_SRC python
      class CBHG(keras.Model):
        def __init__(self, K=16):
          super(CBHG, self).__init__()
          self.conv_banks = [BatchNormConv1D(filters=128,
                                             kernel_size=k,
                                             strides=1,
                                             activation=tf.nn.relu) for k in range(1, K+1)]

        def call(self, inputs):
          x = inputs
          x = keras.layers.concatenate([conv_bank(x) for conv_bank in self.conv_banks])
          return x
    #+END_SRC
    #+BEGIN_NOTES
    With the Conv Bank we are trying to extract out all of the details of the
    sentence from different views. So, in effect, we are looking at the sentence
    as single characters, then character pairs, then triplets, and up from
    there. Sometimes, this type of data is provided as metadata to each of the
    characters, but we can build it directly into our model.

    Notice in the banks we use not just a Conv1D, but a BatchNormConv1D. What's
    that?
    #+END_NOTES
*** Vanishing Gradient
*** Batch Normalization
    "Batch normalization is used for all convolutional layers"

    #+BEGIN_SRC python
      class BatchNormConv1D(keras.Model):
        def __init__(self, filters, kernel_size, strides, activation):
          super(BatchNormConv1D, self).__init__()
          self.conv1D = keras.layers.Conv1D(filters = filters,
                                            kernel_size = kernel_size,
                                            strides = strides,
                                            activation=activation,
                                            padding="same")
          self.bn = keras.layers.BatchNormalization()

        def call(self, x):
          x = self.conv1D(x)
          x = self.bn(x)
          return x
    #+END_SRC

    #+BEGIN_NOTES
    As you add more and more layers to your model, it becomes possible for your
    values to explode or vanish. In order to help prevent this from happening,
    batch normalization is introduced throughout your model to help keep your
    data easy to work with.

    The typical way to do this is by subtracting the average and dividing by the
    standard deviation.
    #+END_NOTES
    
*** Max-Pooling
    #+ATTR_REVEAL: :frag (roll-in)
    - Down-samples data
    - Increases speed of training model
*** Max-Pooling (Example)
    https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png 
*** Max-Pooling (Code)
    #+BEGIN_SRC python
      class CBHG(keras.Model):
        def __init__(self, K=16, projections=[128, 128]):
          super(CBHG, self).__init__()
          self.conv_banks = [BatchNormConv1D(filters=128,
                                             kernel_size=k,
                                             strides=1,
                                             activation=tf.nn.relu) for k in range(1, K+1)]
          self.max_pool_1D = keras.layers.MaxPool1D(strides = 1,      # NEW
                                                    pool_size = 2,    # NEW
                                                    padding = "same") # NEW

        def call(self, inputs):
          x = inputs
          x = keras.layers.concatenate([conv_bank(x) for conv_bank in self.conv_banks])
          x = self.max_pool_1D(x)                                     # NEW
          return x
    #+END_SRC
*** Residual Connection
    Deeper networks are harder to train

    Data becomes saturated
    
    #+BEGIN_NOTES
    As our neural network gets deeper and deeper, the model begins to become
    saturated and it's ability to learn degrades. So, similar to batch
    normalization, we need a way to refresh our data as it goes deeper into our
    model. Residual connections do this by holding onto the original input,
    running it through some neural networks, and then adding the original input
    to the resulting output.

    The resulting output can be of any shape or size, so we need to use a
    projection to bring it back to the shape of the original input.
    #+END_NOTES
*** Residual Connection (Code)
   #+BEGIN_SRC python
     class CBHG(keras.Model):
       def __init__(self, K=16, projections=[128, 128]):
         super(CBHG, self).__init__()
         self.conv_banks = [BatchNormConv1D(filters=128,
                                        kernel_size=k,
                                        strides=1,
                                        activation=tf.nn.relu) for k in range(1, K+1)]
         self.max_pool_1D = keras.layers.MaxPool1D(strides = 1,
                                                   pool_size = 2,
                                                   padding = "same")
         self.conv1d_projections = [                       # NEW
           BatchNormConv1D(128, 3, 1, tf.nn.relu),         # NEW
           BatchNormConv1D(128, 3, 1, "linear")            # NEW
         ]                                                 # NEW

       def call(self, inputs):
         x = inputs
         x = keras.layers.concatenate([conv_bank(x) for conv_bank in self.conv_banks])
         x = self.max_pool_1D(x)
         for conv1d_projection in self.conv1d_projections: # NEW 
           x = conv1d_projection(x)                        # NEW
         # Residual Connection                             # NEW
         x = keras.layers.add([x, inputs])                 # NEW
         return x
   #+END_SRC 
*** Highway Layers
    Similar to Residual Connection, but instead of always adding the original
    with the residual, we learn how much of either should pass through.
*** Bidirectional RNN
   - Learns sequences and ordering (of both directions)
   - Gives the effect of memory by passing it's output along
    
** Attention
   A lense over the context vector

   Learns where to focus
   #+BEGIN_NOTES
   Acts as a lense over the context vector to draw attention to important data
   #+END_NOTES
** Pre-net
** Attention RNN
** Decoder RNN
** CBHG
** Linear Scale Spectrogram
** Griffin-Lim Reconstruction
   
* Tacotron
  [[file:tacotron-architecture.png]] 


* Questions
  - Does the conv net actual train on each of the small pieces?
  - How does it do alignment?
    - Alignment is part of the attention process. Over time the attention is
      able to determine which characters make the different sounds, and then, I
      believe, the RNN sorts out the actual sequence of how those sounds are
      ordered.

* Tacotron - Deep Learning for Voice Synthesis
  [[https://arxiv.org/pdf/1703.10135.pdf][Tacotron Paper]] 
  - Data Pre-processing
  - Embedding
  - Convolutional Neural Networks
  - Recurrent Neural Networks
  - Sequence-to-Sequence with Attention
** Data Pre-processing
   - Text -> Ids
   - WAV -> Spectrograms
   https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png
** Embedding
   - Ids -> Dense Tensor
   - The tensor can be thought of as a point in a n-dimensional space, where
     similar characters (in the context of the model) are moved closer together.
     
   #+REVEAL_HTML: <img width="400" height="400" src="https://www.tensorflow.org/images/tsne.png">
** Convolutional Neural Networks
   - Dense Networks struggle to focus on details
   - Trains on smaller parts
   - Applies filters
   - Typically Paired with a pooling layer
   https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif
** Recurrent Neural Networks
   - Learns sequences and ordering
   - Gives the effect of memory by passing it's output along
** Sequence-2-Sequence with Attention
   #+REVEAL_HTML: <img src="https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg" width="500" alt="attention mechanism">
* Notes
  1. How would I continue this project given more time?
  2. Lessons Learned? Summarizing the knowledge gained?
  3. How have I added to this knowledge base?
  4. What has already been done?
  5. How well did I accomplish my goal?
  6. 


What is convergence?
