#+TITLE: Voice Synthesis with TensorFlow
#+DATE: <2018-09-17 Mon>
#+AUTHOR: Colton Kopsa
#+EMAIL: coljamkop@gmail.com

#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:t
#+OPTIONS: reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:800
#+OPTIONS: toc:1
#+REVEAL_ROOT: file:///home/colton/dev/reveal.js/
#+REVEAL_THEME: moon
#+REVEAL_HLEVEL: 1
#+REVEAL_PLUGINS: (markdown notes)

* Notes
  1. How would I continue this project given more time?
  2. Lessons Learned? Summarizing the knowledge gained?
  3. How have I added to this knowledge base?
  4. What has already been done?
  5. How well did I accomplish my goal?

* About Me
  Colton Kopsa
  
  Clearwater Analytics
  
  BYU-I

* Voice Synthesis
** Concatenation Based Synthesis 
   #+ATTR_REVEAL: :frag (roll-in)
   - Database built from speech-phrase pairs
   - Speech -> unique sounds
   - Phrases -> unique symbols
   - Words -> unique symbols -> unique sounds
   #+BEGIN_NOTES
   This has been drastically simplified.
   Unique sounds (phonemes, diphones, etc)
   Unique symbols are typically accompanied by metadata:
   - Preceding and following phonemes
   - Position of segment in syllable
   - Position of syllable in word & phrase
   - Position of word in phrase
   - Stress/accent/length features of current/preceding/following syllables
   - Distance from stressed/accented syllable
   - POS of current/preceding/following word
   - Length of current/preceding/following phrase
   - End tone of phrase
   - Length of utterance measured in syllables/words/phrases

   The list of unique sounds are stitched together
   #+END_NOTES

** Concatenation Based Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="concat-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Statistical Parametric Synthesis
   #+ATTR_REVEAL: :frag (roll-in)
   - Uses Hidden Markov Models
   - Each generates different parts of the final waveform
   #+BEGIN_NOTES
   Similar to Concatenation
   Database is replaced with a trained model
   #+END_NOTES
    
** Statistical Parametric Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="sp-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Deep Learning Based Synthesis
   #+ATTR_REVEAL: :frag (roll-in)
   - Use the magic of machine learning to make all of your problems easy.

** Deep Learning Based Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="fake-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

* Deep Learning
** Machine Learning (Supervised Learning)
   - Classical Programming vs Machine Learning
     - Classical Programming: Data + Rules => Answers
     - Machine Learning: Data + Answers => Rules
   - There are many different models for machine learning, but we are going to focus on neural networks.
** Neural Networks
   #+REVEAL_HTML: <iframe width="1200" height="600" src="https://www.youtube.com/embed/rEDzUT3ymw4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
** How does it learn?
   1. Data is fed through a neural network 
   2. A loss is computed between the actual targets and what the neural network
      predicted.
   3. An optimizer uses the loss to nudge the weights of the neural network in
      the right direction.
   4. The neural net's accuracy improves.
** Tying it Back To Voice Synthesis 
   - What we have:
     - Text (Data)
     - Audio (Answers)
   - What we want:
     - A way to convert text to audio (Rules)
** Deep Learning
   - ImageNet
     - 1000 categories
     - 1.4 million images
     - 2011 - Classical Approach - 74.3%
     - 2012 - Deep Learning - 83.6%
** ImageNet Results
   [[file:ImageNet%20Results.png]] 
   
** Why is deep learning better?
   - Removes the need for feature engineering.
   - The layers train as one giant model.
* Tacotron - Deep Learning for Voice Synthesis
  [[file:tacotron-architecture.png]] 
** Inputs
   Unique Numerical Representation of Words
   #+BEGIN_SRC python
     def convertWordToIndex:
         # Example code goes here
   #+END_SRC
** Outputs:
   Log-Mel Spectrogram
   #+BEGIN_SRC python
   #+END_SRC
   #+REVEAL: split
   Linear Spectrograms
   #+BEGIN_SRC python
   #+END_SRC

** Character Embeddings
   #+ATTR_REVEAL: :frag (roll-in)
   - Gives spatial significance to the words
   - Map lower dimensional data to a higher dimension
   #+REVEAL: split
   #+REVEAL_HTML: <img width="750" height="750" src="https://www.tensorflow.org/images/tsne.png">
   #+BEGIN_NOTES
   - System and computer are close
   - Data and information are close
   - English and French are close
   #+END_NOTES
   #+REVEAL: split
   #+BEGIN_SRC python
     embedding = keras.layers.Embedding(input_dim=vocab_inp_size,
                                        output_dim=256,
                                        input_length=max_length_inp)
   #+END_SRC
   #+BEGIN_NOTES
   vocab inp size - the total number of unique words in our dataset
   max length inp - the max number of words in a sentence from our dataset
   #+END_NOTES
** "Pre-net"
   "Helps convergence and improves generalization."
   #+BEGIN_SRC python
     class EncoderPrenet(keras.Model):
       def __init__(self):
         super(EncoderPrenet, self).__init__()
         self.dense_1 = keras.layers.Dense(256, activation=tf.nn.relu)
         self.dense_2 = keras.layers.Dense(128, activation=tf.nn.relu)
         self.dropout = tf.keras.layers.Dropout(0.5)

       def call(self, x):
         x = self.dense_1(x)
         x = self.dropout(x)
         x = self.dense_2(x)
         x = self.dropout(x)
         return x
   #+END_SRC
   #+BEGIN_NOTES
   Dropout randomly drop data given a dropout rate. In this case %50. This can
   help generalize because it learns to work without information.
   
   I think that the convergence benefits come with the non-Linear (ReLU) to help
   normalize the data in the model.
   #+END_NOTES
** CBHG
   Convolutional Banks, Highway Network, Bidirectional GRU
   "Powerful module for extracting representations from sequences"
   [[file:cbhg.png]]
   #+BEGIN_NOTES
   Lots of new stuff, stay with me.
   #+END_NOTES
*** Convolutional Neural Networks - CNN
    #+ATTR_REVEAL: :frag (roll-in)
    - Trains on smaller parts
    - Applies filters
    - Typically Paired with a pooling layer
    #+BEGIN_NOTES
    Dense Networks struggle to focus on details
    Conv Nets are a way to focus on details under different lights
    It focuses on details by stepping through the data frame-by-frame
    It gets different lights by applying different filters on the data
    #+END_NOTES
      
*** Convolutional Neural Networks - CNN (Example)
    https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif
    #+BEGIN_NOTES
    The box traversing the image is stepping through the data frame-by-frame
    The images that it generated on the left is the filtered data
    #+END_NOTES
*** Convolutional Neural Networks - CNN (Code)
    #+BEGIN_SRC python
      class CBHG(keras.Model):
        def __init__(self, K=16):
          super(CBHG, self).__init__()
          self.conv_banks = [BatchNormConv1D(filters=128,
                                             kernel_size=k,
                                             strides=1,
                                             activation=tf.nn.relu) for k in range(1, K+1)]

        def call(self, inputs):
          x = inputs
          x = keras.layers.concatenate([conv_bank(x) for conv_bank in self.conv_banks])
          return x
    #+END_SRC
    #+BEGIN_NOTES
    With the Conv Bank we are trying to extract out all of the details of the
    sentence from different views. So, in effect, we are looking at the sentence
    as single words, then word pairs, then triplets, and up from there.

    Notice in the banks we use not just a Conv1D, but a BatchNormConv1D. What's
    that?
    #+END_NOTES
*** Batch Normalization
    "Batch normalization is used for all convolutional layers"

    #+BEGIN_SRC python
      class BatchNormConv1D(keras.Model):
        def __init__(self, filters, kernel_size, strides, activation):
          super(BatchNormConv1D, self).__init__()
          self.conv1D = keras.layers.Conv1D(filters = filters,
                                            kernel_size = kernel_size,
                                            strides = strides,
                                            activation=activation,
                                            padding="same")
          self.bn = keras.layers.BatchNormalization()

        def call(self, x):
          x = self.conv1D(x)
          x = self.bn(x)
          return x
    #+END_SRC

    #+BEGIN_NOTES
    As you add more and more layers to your model, it becomes possible for your
    values to explode or vanish. In order to help prevent this from happening,
    batch normalization is introduced throughout your model to help keep your
    data easy to work with.

    The typical way to do this is by subtracting the average and dividing by the
    standard deviation.
    #+END_NOTES
    
*** Max-Pooling
    #+ATTR_REVEAL: :frag (roll-in)
    - Down-samples data
    - Increases speed of training model
*** Max-Pooling (Example)
    https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png 
*** Max-Pooling (Code)
    #+BEGIN_SRC python
      class CBHG(keras.Model):
        def __init__(self, K=16, projections=[128, 128]):
          super(CBHG, self).__init__()
          self.conv_banks = [BatchNormConv1D(filters=128,
                                             kernel_size=k,
                                             strides=1,
                                             activation=tf.nn.relu) for k in range(1, K+1)]
          self.max_pool_1D = keras.layers.MaxPool1D(strides = 1,      # NEW
                                                    pool_size = 2,    # NEW
                                                    padding = "same") # NEW

        def call(self, inputs):
          x = inputs
          x = keras.layers.concatenate([conv_bank(x) for conv_bank in self.conv_banks])
          x = self.max_pool_1D(x)                                     # NEW
          return x
    #+END_SRC
*** Residual Connection
    Deeper networks are harder to train

    Data becomes saturated
    
    #+BEGIN_NOTES
    As our neural network gets deeper and deeper, the model begins to become
    saturated and it's ability to learn degrades. So, similar to batch
    normalization, we need a way to refresh our data as it goes deeper into our
    model. Residual connections do this by holding onto the original input,
    running it through some neural networks, and then adding the original input
    to the resulting output.

    The resulting output can be of any shape or size, so we need to use a
    projection to bring it back to the shape of the original input.
    #+END_NOTES
*** Residual Connection (Code)
   #+BEGIN_SRC python
     class CBHG(keras.Model):
       def __init__(self, K=16, projections=[128, 128]):
         super(CBHG, self).__init__()
         self.conv_banks = [BatchNormConv1D(filters=128,
                                        kernel_size=k,
                                        strides=1,
                                        activation=tf.nn.relu) for k in range(1, K+1)]
         self.max_pool_1D = keras.layers.MaxPool1D(strides = 1,
                                                   pool_size = 2,
                                                   padding = "same")
         self.conv1d_projections = [                       # NEW
           BatchNormConv1D(128, 3, 1, tf.nn.relu),         # NEW
           BatchNormConv1D(128, 3, 1, "linear")            # NEW
         ]                                                 # NEW

       def call(self, inputs):
         x = inputs
         x = keras.layers.concatenate([conv_bank(x) for conv_bank in self.conv_banks])
         x = self.max_pool_1D(x)
         for conv1d_projection in self.conv1d_projections: # NEW 
           x = conv1d_projection(x)                        # NEW
         # Residual Connection                             # NEW
         x = keras.layers.add([x, inputs])                 # NEW
         return x
   #+END_SRC 
*** Highway Layers
    Similar to Residual Connection, but instead of always adding the original
    with the residual, we learn how much of either should pass through.
*** Bidirectional RNN
   - Learns sequences and ordering (of both directions)
   - Gives the effect of memory by passing it's output along
    
** Attention
** Pre-net
** Attention RNN
** Decoder RNN
** CBHG
** Linear Scale Spectrogram
** Griffin-Lim Reconstruction
   
* Tacotron
  [[file:tacotron-architecture.png]] 


* Questions
  - Does the conv net actual train on each of the small pieces?
  - How does it do alignment?
    - Alignment is part of the attention process. Over time the attention is
      able to determine which characters make the different sounds, and then, I
      believe, the RNN sorts out the actual sequence of how those sounds are
      ordered.

* Tacotron - Deep Learning for Voice Synthesis
  [[https://arxiv.org/pdf/1703.10135.pdf][Tacotron Paper]] 
  - Data Pre-processing
  - Embedding
  - Convolutional Neural Networks
  - Recurrent Neural Networks
  - Sequence-to-Sequence with Attention
** Data Pre-processing
   - Text -> Ids
   - WAV -> Spectrograms
   https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png
** Embedding
   - Ids -> Dense Tensor
   - The tensor can be thought of as a point in a n-dimensional space, where
     similar words (in the context of the model) are moved closer together.
     
   #+REVEAL_HTML: <img width="400" height="400" src="https://www.tensorflow.org/images/tsne.png">
** Convolutional Neural Networks
   - Dense Networks struggle to focus on details
   - Trains on smaller parts
   - Applies filters
   - Typically Paired with a pooling layer
   https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif
** Recurrent Neural Networks
   - Learns sequences and ordering
   - Gives the effect of memory by passing it's output along
** Sequence-2-Sequence with Attention
   #+REVEAL_HTML: <img src="https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg" width="500" alt="attention mechanism">
