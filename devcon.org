#+TITLE: Voice Synthesis with TensorFlow
#+AUTHOR: Colton Kopsa
#+EMAIL: coljamkop@gmail.com

#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:t
#+OPTIONS: reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:800
#+OPTIONS: toc:1
#+REVEAL_ROOT: file:///home/colton/dev/reveal.js/
#+REVEAL_THEME: moon
#+REVEAL_HLEVEL: 1
#+REVEAL_PLUGINS: (markdown notes)

* About Me
  Colton Kopsa
  
  Clearwater Analytics
  
  BYU-I
  #+BEGIN_NOTES
  Software Developer

  Novice in Deep Learning/Machine Learning

  Spent a good amount of time learning about this subject
  
  I may not be able to answer all questions

  Feel free to correct me
  #+END_NOTES

* Voice Synthesis
  - Concatenation Based Synthesis
  - Statistical Parametric Synthesis
  - Deep Learning Based Synthesis
** Concatenation Based Synthesis 
   #+ATTR_REVEAL: :frag (roll-in)
   - Speech -> unique sounds
   - Phrases -> unique symbols
   - Words -> unique symbols -> unique sounds
   #+BEGIN_NOTES
   - This has been drastically simplified.
   - Dataset built from audio-sentence(phrase) pairs
   - Unique sounds (phonemes, diphones, etc) that are a part of our speech
   - Unique symbols are typically accompanied by metadata:
     - Preceding and following phonemes
     - Position of segment in syllable
     - Position of syllable in word & phrase
     - Position of word in phrase
     - Stress/accent/length features of current/preceding/following syllables
     - Distance from stressed/accented syllable
     - POS of current/preceding/following word
     - Length of current/preceding/following phrase
     - End tone of phrase
     - Length of utterance measured in syllables/words/phrases

   The list of unique sounds are stitched together
   #+END_NOTES

** Concatenation Based Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="concat-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Statistical Parametric Synthesis
   #+ATTR_REVEAL: :frag (roll-in)
   - Uses Hidden Markov Models
   - Each generates different parts of the final waveform
   #+BEGIN_NOTES
   Similar to Concatenation
   Database is replaced with a trained model
   #+END_NOTES
    
** Statistical Parametric Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="sp-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Deep Learning Based Synthesis
   #+ATTR_REVEAL: :frag (roll-in)
   - Machine Learning Magic

** Deep Learning Based Synthesis (Example)
   #+REVEAL_HTML: <audio controls="controls"> <source src="deep-sample.wav" type="audio/wav"> Your browser does not support the <code>audio</code> element. </audio>

** Questions?
* What is Deep Learning?
  Machine Learning -> Neural Networks -> Deep Learning
** Machine Learning (Supervised Learning)
   - Classical Programming vs Machine Learning
     - Classical Programming: Data + Rules => Answers
     - Machine Learning: Data + Answers => Rules
** Neural Networks
   #+REVEAL_HTML: <iframe width="1200" height="600" src="https://www.youtube.com/embed/rEDzUT3ymw4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
** How does it learn?
   1. Data -> NN -> Prediction
   2. Loss = Prediction - Truth
   3. Optimizer uses loss to nudge weights in the right direction.
   4. Accuracy improves.
** Deep Learning
   Neural Network with many hidden layers
** ImageNet
   #+ATTR_REVEAL: :frag (roll-in)
     - 14 million images
     - 20,000+ categories
     - 2011 - Classical Approach - 74.3%
     - 2012 - Deep Learning - 83.6%
     #+BEGIN_NOTES
     - ImageNet was a competition that popularized deep learning
     - AlexNet blew the previous competition out of the water
     - Deep Learning has ruled the competition since
     #+END_NOTES

** ImageNet Results
   [[file:ImageNet%20Results.png]] 
   #+BEGIN_NOTES
   - 2010 and 2011 used shallow methods
   - 2012 is when AlexNet won with 8 layers
   - 2013 was similar to AlexNet but with improved training
   - 2014 moves to 22 layers with a jump of 5%
   - 2015 jumps 3% with 152 layers
   #+END_NOTES
   
** Why is deep learning better?
   #+ATTR_REVEAL: :frag (roll-in)
   - More layers = More capability to memorize
   - No Feature Engineering
   - Train as one model
   #+BEGIN_NOTES
   - As trainable weight increase it's ability to memorize the data it's
     training on improves. The more layers the better.
   - Feature engineering is used with other machine learning methods
     - A models ability to infer is greatly influenced by the data it trains on
     - Including irrelevant data can cause the models ability to infer to
       degrade
     - The person training the model has to deliberately choose which features
       to include/discard
     - Neural networks learn which features to stress/ignore while it adjusts
       its weights
   #+END_NOTES

** Tying it Back To Voice Synthesis
   - What we have:
     - Text (Data)
     - Audio (Answers)
   - What we want:
     - A way to convert text to audio (Rules)
   #+BEGIN_NOTES
   Other voice synthesis processes usually accompany the text data with
   metadata, this model will operate without metadata.
   #+END_NOTES
** Questions?
* Tacotron - Deep Learning for Voice Synthesis
  [[file:tacotron-architecture.png]] 
** Inputs
   Unique Id Representation of Characters

   'a' -> 1, 'b' -> 2
** Outputs:
   Log-Mel Spectrograms
   
   Linear Spectrograms

   https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png

** Character Embeddings
   #+ATTR_REVEAL: :frag (roll-in)
   - Gives spatial significance to the characters
   - Map lower dimensional data to a higher dimension
   #+REVEAL: split
   #+REVEAL_HTML: <img width="750" height="750" src="https://www.tensorflow.org/images/tsne.png">
   #+BEGIN_NOTES
   - System and computer are close
   - Data and information are close
   - English and French are close
   #+END_NOTES
   #+REVEAL: split
   #+BEGIN_SRC python
     embedding = keras.layers.Embedding(input_dim=vocab_inp_size,
                                        output_dim=256,
                                        input_length=max_length_inp)
   #+END_SRC
   #+BEGIN_NOTES
   vocab inp size - the total number of unique characters in our dataset
   max length inp - the max number of characters in a sentence from our dataset
   #+END_NOTES
** "Pre-net"
   "Helps convergence and improves generalization."
   #+BEGIN_SRC python
     class EncoderPrenet(keras.Model):
       def __init__(self):
         super(EncoderPrenet, self).__init__()
         self.dense_1 = keras.layers.Dense(256, activation=tf.nn.relu)
         self.dense_2 = keras.layers.Dense(128, activation=tf.nn.relu)
         self.dropout = tf.keras.layers.Dropout(0.5)

       def call(self, x):
         x = self.dense_1(x)
         x = self.dropout(x)
         x = self.dense_2(x)
         x = self.dropout(x)
         return x
   #+END_SRC
   #+BEGIN_NOTES
   Dense (or fully connected) layers receive output from all of the nodes in the
   previous layer

   Dropout randomly drop data given a dropout rate. In this case %50. This can
   help generalize because it learns to work without information.
   
   The drop from 256 nodes to 128 is to provide a bottleneck.
   #+END_NOTES
** CBHG
   Convolutional Bank, Highway Network, Bidirectional GRU
   "Powerful module for extracting representations from sequences"
   [[file:cbhg.png]]
   #+BEGIN_NOTES
   Lots of new stuff, stay with me.
   #+END_NOTES
*** Convolutional Neural Networks - CNN
    #+ATTR_REVEAL: :frag (roll-in)
    - Trains on smaller parts
    - Applies filters
    #+BEGIN_NOTES
    - Dense Networks struggle to focus on details
    - Conv Nets are a way to focus on details under different lights
    - It focuses on details by stepping through the data frame-by-frame
    - It gets different lights by applying different filters on the data
    #+END_NOTES
      
*** Convolutional Neural Networks - CNN (Example)
    https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif
    #+BEGIN_NOTES
    The box traversing the image is stepping through the data frame-by-frame
    The images that it generated on the left is the filtered data
    #+END_NOTES
*** Convolutional Neural Networks - CNN (Code)
    #+BEGIN_SRC python
      class CBHG(keras.Model):
        def __init__(self, K=16):
          super(CBHG, self).__init__()
          self.conv_bank = [BatchNormConv1D(filters=128,
                                             kernel_size=k,
                                             strides=1,
                                             activation=tf.nn.relu) for k in range(1, K+1)]

        def call(self, inputs):
          x = inputs
          x = keras.layers.concatenate([conv_layer(x) for conv_layer in self.conv_bank])
          return x
    #+END_SRC
    #+BEGIN_NOTES
    With the Conv Bank we are trying to extract out all of the details of the
    sentence from different views. So, in effect, we are looking at the sentence
    as single characters, then character pairs, then triplets, and up from
    there. Sometimes, this type of data is provided as metadata to each of the
    characters, but we can build it directly into our model.

    Notice in the banks we use not just a Conv1D, but a BatchNormConv1D. What's
    that?
    #+END_NOTES
*** Vanishing/Exploding Gradient
    
    #+BEGIN_NOTES
    
    #+END_NOTES
*** Batch Normalization
    "Batch normalization is used for all convolutional layers"

    #+BEGIN_SRC python
      class BatchNormConv1D(keras.Model):
        def __init__(self, filters, kernel_size, strides, activation):
          super(BatchNormConv1D, self).__init__()
          self.conv1D = keras.layers.Conv1D(filters = filters,
                                            kernel_size = kernel_size,
                                            strides = strides,
                                            activation=activation,
                                            padding="same")
          self.bn = keras.layers.BatchNormalization()

        def call(self, x):
          x = self.conv1D(x)
          x = self.bn(x)
          return x
    #+END_SRC

    #+BEGIN_NOTES
    As you add more and more layers to your model, it becomes possible for your
    values to explode or vanish. In order to help prevent this from happening,
    batch normalization is introduced throughout your model to help keep your
    data easy to work with.

    The typical way to do this is by subtracting the average and dividing by the
    standard deviation.
    #+END_NOTES
    
*** Max-Pooling
    #+ATTR_REVEAL: :frag (roll-in)
    - Down-samples data
    - Increases speed of training model
*** Max-Pooling (Example)
    https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png 
*** Max-Pooling (Code)
    #+BEGIN_SRC python
      class CBHG(keras.Model):
        def __init__(self, K=16, projections=[128, 128]):
          super(CBHG, self).__init__()
          self.conv_bank = [BatchNormConv1D(filters=128,
                                             kernel_size=k,
                                             strides=1,
                                             activation=tf.nn.relu) for k in range(1, K+1)]
          self.max_pool_1D = keras.layers.MaxPool1D(strides = 1,      # NEW
                                                    pool_size = 2,    # NEW
                                                    padding = "same") # NEW

        def call(self, inputs):
          x = inputs
          x = keras.layers.concatenate([conv_layer(x) for conv_layer in self.conv_bank])
          x = self.max_pool_1D(x)                                     # NEW
          return x
    #+END_SRC
*** Residual Connection
    #+ATTR_REVEAL: :frag (roll-in)
    - Deeper networks are harder to train
    - Data becomes saturated
    
    #+BEGIN_NOTES
    As our neural network gets deeper and deeper, the model begins to become
    saturated and it's ability to learn degrades. So, similar to batch
    normalization, we need a way to refresh our data as it goes deeper into our
    model. Residual connections do this by holding onto the original input,
    running it through some neural networks, and then adding the original input
    to the resulting output.

    The resulting output can be of any shape or size, so we need to use a
    projection to bring it back to the shape of the original input.
    #+END_NOTES
*** Residual Connection (Code)
   #+BEGIN_SRC python
     class CBHG(keras.Model):
       def __init__(self, K=16, projections=[128, 128]):
         super(CBHG, self).__init__()
         self.conv_bank = [BatchNormConv1D(filters=128,
                                        kernel_size=k,
                                        strides=1,
                                        activation=tf.nn.relu) for k in range(1, K+1)]
         self.max_pool_1D = keras.layers.MaxPool1D(strides = 1,
                                                   pool_size = 2,
                                                   padding = "same")
         self.conv1d_projections = [                       # NEW
           BatchNormConv1D(128, 3, 1, tf.nn.relu),         # NEW
           BatchNormConv1D(128, 3, 1, "linear")            # NEW
         ]                                                 # NEW

       def call(self, inputs):
         x = inputs
         x = keras.layers.concatenate([conv_layer(x) for conv_layer in self.conv_bank])
         x = self.max_pool_1D(x)
         for conv1d_projection in self.conv1d_projections: # NEW 
           x = conv1d_projection(x)                        # NEW
         # Residual Connection                             # NEW
         x = keras.layers.add([x, inputs])                 # NEW
         return x
   #+END_SRC 
*** Highway Layers
    #+ATTR_REVEAL: :frag (roll-in)
    - Similar to Residual Connection
    - Learn how much of the original/residual to pass on
*** Bidirectional RNN
    #+ATTR_REVEAL: :frag (roll-in)
    - Learns sequences and ordering (bidirectional = both directions)
    - Gives the effect of memory by passing it's output along
*** Bidirectional RNN (Example)
   | RNN:     | T | h | e |   | d | o | ? |
   | Not RNN: |   |   |   |   |   | o | ? |
    
** Context Vector (Simplified)
   [[file:context-vector.png]]  
** Attention
   #+ATTR_REVEAL: :frag (roll-in)
   - A lens over the context vector
   - Learns where to focus
   #+BEGIN_NOTES
   Acts as a lens over the context vector to draw attention to important data
   #+END_NOTES
** Attention (Example)
   [[file:context-vector.png]]  
** "Pre-net"
   Same as pre-net in encoder
** Attention RNN
   #+ATTR_REVEAL: :frag (roll-in)
   - Combine attention and log-mel spectrograms
   - Pass attention and output of attention RNN to Decoder RNN
** Decoder RNN
   Outputs log-mel spectrograms
** CBHG
   Acts as a post processing network
   
   Outputs Linear-scale spectrograms
** Griffin-Lim Reconstruction
   Converts linear spectrograms to waveforms
** Generating Audio
   #+ATTR_REVEAL: :frag (roll-in)
   - Feed character sequence to encoder and get context vector
   - Feed decoder <GO> frame and context vector
   - Stitch output from decoder
** Next Steps
   - For me:
     - Finish Code
     - Create my own dataset
   - For you:
     - Find a interesting paper
     - Learn a Deep Learning Framework
     - Reproduce Results
** Questions?
* Tacotron
  [[file:tacotron-architecture.png]] 

